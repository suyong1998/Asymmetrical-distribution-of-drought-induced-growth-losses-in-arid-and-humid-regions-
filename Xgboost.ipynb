{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1964ec-5432-4ef5-b496-2884e6295316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf7ebf-456a-486c-bbc6-c08c4a7c6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(true, pred):\n",
    "    diff = np.abs(np.array(true) - np.array(pred))\n",
    "    return np.mean(diff / abs(true))\n",
    "df_gro = pd.read_csv('xgboost95.csv')\n",
    "\n",
    "columns_to_standardize = df_gro.columns.difference(['site', 'Growth_loss','AI_Class'])\n",
    "scaler = StandardScaler()\n",
    "df_gro[columns_to_standardize] = scaler.fit_transform(df_gro[columns_to_standardize])\n",
    "grouped = df_gro.groupby('AI_Class')\n",
    "group_humid = grouped.get_group('Humid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37b8b5-856e-40a8-961c-e7919c49ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_arid = grouped.get_group('Arid')\n",
    "group_arid = group_arid.drop(['site','AI_Class','Temp','Tmax','Pet','Tmin'],axis=1)\n",
    "spearman_corr = group_arid.corr(method='spearman')\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "input_features = group_arid.drop(columns=['Growth_loss'])\n",
    "vif_df = calculate_vif(input_features)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(round(spearman_corr,2), annot=True, cmap='RdBu')\n",
    "plt.title('Arid Region')\n",
    "\n",
    "plt.savefig('FigS9A.jpg', dpi=800, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "vif_df.to_csv('Arid_VIF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0d35c-88ed-4edd-b73c-79c65954d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "X = group_arid.loc[:, group_arid.columns != 'Growth_loss']\n",
    "y = group_arid['Growth_loss']\n",
    "model = xgb.XGBRegressor(max_depth=4, learning_rate=0.07, n_estimators=100, min_child_weight=7, subsample=0.8, colsample_bytree=0.7, reg_lambda=0.4, random_state=30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs = -1)\n",
    "print(np.sqrt(-scores.mean()))\n",
    "print(\"Training R2:\", r2_score(y_train, Z_1), \"RMSE:\", np.sqrt(mean_squared_error(y_train, Z_1)), \"MAE:\", mean_absolute_error(y_train, Z_1), \"MAPE:\", MAPE(y_train, Z_1)*100,\"%\")\n",
    "print(\"Testing R2:\", r2_score(y_test, Z_2), \"RMSE:\", np.sqrt(mean_squared_error(y_test, Z_2)), \"MAE:\", mean_absolute_error(y_test, Z_2), \"MAPE:\", MAPE(y_test,Z_2)*100,\"%\")\n",
    "Z_3 = model.predict(X)\n",
    "\n",
    "xx = np.zeros(shape=(len(X),2))\n",
    "xx[:,0]=y\n",
    "xx[:,1]=Z_3\n",
    "np.savetxt('XGB.csv', xx, delimiter = ',') \n",
    "# plot the shear strength prediction results\n",
    "xx = np.linspace(-0.5, 0.5)\n",
    "yy = xx\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(5,4.5),dpi=100)\n",
    "plt.plot(xx, yy, linewidth=2,c='red')\n",
    "plt.scatter(y_train, Z_1, marker='o')\n",
    "plt.scatter(y_test, Z_2, marker='s')\n",
    "\n",
    "plt.tick_params (axis='both',which='major',labelsize=18)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = 18)\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = 18)\n",
    "\n",
    "font1 = {'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 20,}\n",
    "font2 = {'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 15,}\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Tested Growth loss', font1)\n",
    "plt.xticks([-0.5,0,0.5])\n",
    "plt.yticks([-0.5,0,0.5])\n",
    "plt.ylabel('Predicted Growth loss', font1)\n",
    "plt.text(0,0.4,'Arid region', font2)\n",
    "plt.text(0.1,-0.4,'Training '+\"R\\u00B2\" +'= 0.78', font2)\n",
    "plt.text(0.1,-0.3,'Testing '+ \"R\\u00B2\"+'= 0.58', font2)\n",
    "plt.legend(['y = x','Training','Testing'], loc = 'upper left', prop={'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 14,})\n",
    "plt.savefig('FigS10a.jpg', dpi=600, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "X_shap = pd.DataFrame(X, columns = X.columns)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "print(shap_values.shape)\n",
    "np.savetxt('shap values.csv', shap_values, delimiter = ',') \n",
    "# visualize the j-th prediction's explanation and then the entire set\n",
    "y_base = explainer.expected_value\n",
    "group_arid['pred'] = model.predict(X)\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[2], X.iloc[2], matplotlib=True)\n",
    "shap.force_plot(explainer.expected_value, shap_values[16], X.iloc[16], matplotlib=True)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
    "percentages = mean_abs_shap_values / np.sum(mean_abs_shap_values)\n",
    "data = pd.DataFrame({\n",
    "    'feature': X_shap.columns,\n",
    "    'shap_value': mean_abs_shap_values,\n",
    "    'percentage': percentages\n",
    "})\n",
    "\n",
    "data = data.sort_values(by='shap_value', ascending=True)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 10))\n",
    "plt.subplots_adjust(wspace=0.001)\n",
    "ax1.barh(data['feature'], data['shap_value'], color='lightblue')\n",
    "for i, (feature, shap_value, percentage) in enumerate(data.itertuples(index=False)):\n",
    "    ax1.text(shap_value + 0.001, i, f\"{shap_value:.3f}\", va='center', color='blue')\n",
    "    ax1.text(shap_value + 0.005, i, f\"({percentage:.2%})\", va='center', color='red')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.set_xlabel('Mean(SHAP value)',fontsize=14)\n",
    "shap.summary_plot(shap_values, X_shap, show=False)\n",
    "plt.savefig('Fig6a.jpg', dpi=800, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0111564-4079-47d4-b9bf-bf8f7a2b0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_humid = grouped.get_group('Humid')\n",
    "group_humid = group_humid.drop(['site','AI_Class','Tmax','Tmin','Pet','Temp'],axis=1)\n",
    "spearman_corr = group_humid.corr(method='spearman')\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "input_features = group_humid.drop(columns=['Growth_loss'])\n",
    "vif_df = calculate_vif(input_features)\n",
    "vif_df.to_csv('Huimd_VIF.csv',index=False)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(round(spearman_corr,2), annot=True, cmap='RdBu')\n",
    "plt.title('Humid Region')\n",
    "plt.savefig('FigS9b.jpg', dpi=800, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "shap.initjs()\n",
    "model = xgb.XGBRegressor(max_depth=4, learning_rate=0.07, n_estimators=100, min_child_weight=3, subsample=0.8, colsample_bytree=0.6, reg_lambda=1, random_state=30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs = -1)\n",
    "print(np.sqrt(-scores.mean()))\n",
    "Z_1 = model.predict(X_train)\n",
    "Z_2 = model.predict(X_test)\n",
    "print(\"Training R2:\", r2_score(y_train, Z_1), \"RMSE:\", np.sqrt(mean_squared_error(y_train, Z_1)), \"MAE:\", mean_absolute_error(y_train, Z_1), \"MAPE:\", MAPE(y_train, Z_1)*100,\"%\")\n",
    "print(\"Testing R2:\", r2_score(y_test, Z_2), \"RMSE:\", np.sqrt(mean_squared_error(y_test, Z_2)), \"MAE:\", mean_absolute_error(y_test, Z_2), \"MAPE:\", MAPE(y_test,Z_2)*100,\"%\")\n",
    "Z_3 = model.predict(X)\n",
    "\n",
    "xx = np.zeros(shape=(len(X),2))\n",
    "xx[:,0]=y\n",
    "xx[:,1]=Z_3\n",
    "np.savetxt('XGB.csv', xx, delimiter = ',') \n",
    "xx = np.linspace(-0.5, 0.5)\n",
    "yy = xx\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(5,4.5),dpi=100)\n",
    "plt.plot(xx, yy, linewidth=2,c='red')\n",
    "plt.scatter(y_train, Z_1, marker='o')\n",
    "plt.scatter(y_test, Z_2, marker='s')\n",
    "\n",
    "plt.tick_params (axis='both',which='major',labelsize=18)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = 18)\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = 18)\n",
    "\n",
    "font1 = {'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 20,}\n",
    "font2 = {'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 15,}\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Tested Growth loss', font1)\n",
    "plt.ylabel('Predicted Growth loss', font1)\n",
    "plt.yticks([-0.5,0,0.5])\n",
    "plt.xticks([-0.5,0,0.5])\n",
    "plt.text(0,0.4,'Humid region', font2)\n",
    "plt.text(0.1,-0.4,'Training '+\"R\\u00B2\" +'= 0.53', font2)\n",
    "plt.text(0.1,-0.3,'Testing '+ \"R\\u00B2\"+'= 0.32', font2)\n",
    "plt.legend(['y = x','Training','Testing'], loc = 'upper left', prop={'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 14,})\n",
    "\n",
    "plt.savefig('FigS10b.jpg', dpi=600, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "X_shap = pd.DataFrame(X, columns = X.columns)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "print(shap_values.shape)\n",
    "np.savetxt('shap values.csv', shap_values, delimiter = ',') \n",
    "y_base = explainer.expected_value\n",
    "group_humid['pred'] = model.predict(X)\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[2], X.iloc[2], matplotlib=True)\n",
    "shap.force_plot(explainer.expected_value, shap_values[16], X.iloc[16], matplotlib=True)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
    "percentages = mean_abs_shap_values / np.sum(mean_abs_shap_values)\n",
    "data = pd.DataFrame({\n",
    "    'feature': X_shap.columns,\n",
    "    'shap_value': mean_abs_shap_values,\n",
    "    'percentage': percentages\n",
    "})\n",
    "\n",
    "data = data.sort_values(by='shap_value', ascending=True)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 10))\n",
    "plt.subplots_adjust(wspace=0.001)\n",
    "ax1.barh(data['feature'], data['shap_value'], color='lightblue')\n",
    "for i, (feature, shap_value, percentage) in enumerate(data.itertuples(index=False)):\n",
    "    ax1.text(shap_value + 0.001, i, f\"{shap_value:.3f}\", va='center', color='blue')\n",
    "    ax1.text(shap_value + 0.004, i, f\"({percentage:.2%})\", va='center', color='red')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.set_xlabel('Mean(SHAP value)',fontsize=14)\n",
    "shap.summary_plot(shap_values, X_shap, show=False)\n",
    "plt.savefig('Fig6b.jpg', dpi=800, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f5509-1068-495c-882d-7da64d80f0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
